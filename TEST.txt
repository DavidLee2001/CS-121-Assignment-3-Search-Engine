The first 10 search queries performed poorly at first both in terms of ranking performance and runtime performance. The query results were not entirely relevant and the response time was â‰¥ 300ms for each of these terms.


  Poor:
  1) Master of software engineering
  2) To be or not to be
  3) President of the United States
  4) Information and Computer Sciences
  5) Business and finance
  6) University of California, Irvine
  7) Faculty and staff
  8) ICS undergraduate affairs
  9) Being yourself is the best
  10) Donald Bren School
  Well:
  1) Cristina Lopes
  2) David Eppstein
  3) Eppstein Wikipedia
  4) ICS
  5) Donald Bren Hall
  6) Privacy policy
  7) Foo
  8) UCI
  9) Pattis
  10) Computer Sciences


  What we did to optimize:

  1) We accounted for relevant HTML tags to give important text a higher scores so that query results are more relevant:
    regular tags: weight of 1
    <i>, <em> tags: weight of 2
    <b> tags: weight of 3
    <h1> tags: weight of 5
    <h2> tags: weight of 4
    <h3> tags: weight of 3
    <h4> tags: weight of 3
    <h5> tags: weight of 2
    <h6> tags: weight of 2
    These weights are used for the term frequency. For example, for a bolded text (with a weight of 3), it counts as 3 occurrences.

  2) We also used tf-idf to rank the documents to improve ranking performance.

  3) We indexed the index by creating a json  (term_position.json) while merging the partial indexes. This json stores all the terms and their position in the merged index. In doing so, we only needed to read the postings of the query terms as opposed to scanning the file for the query terms.

  4) We generated a champion list of the top 2000 documents,based on tf-idf, for each term while merging. This significantly improved response time. 

